{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoinDesk Web Crawler using Scrapy library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "[Scrapy](https://doc.scrapy.org/en/latest/index.html) is a powerful webscraping library for Python which allows for rapid development of web crawlers. It is a high level framework which provides many features otherwise absent in libraries like BeautifulSoup.\n",
    "\n",
    "Today, we are going to create a web crawler to crawl the Bitcoin news site [Coindesk](https://www.coindesk.com/). Not only are we going to crawl the site, we are going to insert the link, title, date published, time published, and author of each article into a NoSQL database. And not just any database, but Amazon Web Service DyanmoDB.\n",
    "\n",
    "\n",
    "![CoinDesk Logo](https://www.coindesk.com/wp-content/themes/coindesk2/images/footer-logo-square.png)\n",
    "![AWS DyanmoDB Logo](https://upload.wikimedia.org/wikipedia/commons/f/fd/DynamoDB.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Create a New Scrapy Project\n",
    "\n",
    "1. Download Scrapy package\n",
    "2. Type `scrapy startproject coincrawler`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directory and Files \n",
    "Item pipelines are contained in the pipelines.py file and processes items after they have been scraped by your spider. Item Pipelines process these scraped items through several components that are executed sequentially. \n",
    "\n",
    "These processes include:\n",
    " `open_spider()`\n",
    " `close_spider()`\n",
    " `process_item()`\n",
    "\n",
    "Common uses for Item pipelines include:\n",
    "\n",
    "* cleansing HTML data\n",
    "* validating scraped data (checking that the items contain certain fields)\n",
    "* checking for duplicates (and dropping them)\n",
    "* storing the scraped item in a database\n",
    "\n",
    "Also, Don't forget to add your pipeline to the ITEM_PIPELINES setting!!!\n",
    "\n",
    "See: [Scrapy Official Documentation](https://doc.scrapy.org/en/latest/topics/item-pipeline.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the Other Files?\n",
    "nothing changes in settings\n",
    "pipelines.py contains all the processing of one page/item\n",
    "Middlewares dont change\n",
    "init does not change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapy Interpreter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DyanmoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Our Spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class CoinDeskCrawler(scrapy.Spider):\n",
    "    name = \"CoinDesk\"\n",
    "    max_pagenum = 924 #924 \n",
    "    mainpath = 'https://www.coindesk.com/page/'\n",
    "    start_urls = [mainpath + '{}/'.format(i) for i in range(1,max_pagenum+1)]\n",
    "    \n",
    "    def parse(self, response):\n",
    "        # follow links to article pages\n",
    "        for href in response.css('a.fade::attr(href)'):\n",
    "            yield response.follow(href, self.parse_article)\n",
    "            \n",
    "    def parse_article(self, response):\n",
    "        #Get name of url you are opening\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = 'coindesk-%s.txt' % page\n",
    "        yield {\n",
    "            'link': response.url,\n",
    "            'title': str(response.css('title::text').extract_first()),\n",
    "            'date_published': str(response.xpath(\"//meta[@property='article:published_time']/@content\")[1].extract()).split(\"T\")[0],\n",
    "            'time_published': str(response.xpath(\"//meta[@property='article:published_time']/@content\")[1].extract()).split(\"T\")[1],\n",
    "            'author': str(response.css('div.article-top-author-block-right-upper').css('a::text').extract_first()),\n",
    "            }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
